{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZpqIK8YVYuR"
   },
   "source": [
    "## **DATA SCIENCE ASSIGNMENT**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb1gOyCYl6Pv"
   },
   "source": [
    "FIRST WE WILL INSTALL AND IMPORT NECCESAARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 6018,
     "status": "ok",
     "timestamp": 1725480024607,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "gFYVntuvVMrS",
    "outputId": "d172e646-3715-4cd6-9ae5-de7a3bf74010"
   },
   "outputs": [],
   "source": [
    "!pip install deep_sort_realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 3104,
     "status": "ok",
     "timestamp": 1725480027708,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "_pOj3UODVZSh",
    "outputId": "ed7fc461-15c3-4a5c-c0a8-a44b692e03fe"
   },
   "outputs": [],
   "source": [
    "!pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4417,
     "status": "ok",
     "timestamp": 1725480032120,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "mZ2G6gf0U1XY"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 2886,
     "status": "ok",
     "timestamp": 1725480034987,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "RH0yNM0YVfPL",
    "outputId": "0bd77bcd-0864-4850-fa50-f44f42da928e"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1725480034987,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "bUmJuQw5V5Wk",
    "outputId": "cbe70969-11b9-4324-829e-b69ef485b06c"
   },
   "outputs": [],
   "source": [
    "%cd yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2146
    },
    "executionInfo": {
     "elapsed": 67647,
     "status": "ok",
     "timestamp": 1725480102630,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "uksOoIGiV9ae",
    "outputId": "5eee5749-ed35-49ab-a390-a84c5323a950"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 3195,
     "status": "ok",
     "timestamp": 1725480105789,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "cPIi1zLb_A8y",
    "outputId": "0e5c7842-9168-4b6b-b1af-8451d1c63329"
   },
   "outputs": [],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 2655,
     "status": "ok",
     "timestamp": 1725480111570,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "XhVAFzpdRv6B",
    "outputId": "fb0b7488-9e3b-4c3b-a6d1-f01191200efc"
   },
   "outputs": [],
   "source": [
    "   from ultralytics import YOLO\n",
    "\n",
    "   model = YOLO('yolov8n.pt')  # We are using YOLOv8 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSJlk1ldmVmg"
   },
   "source": [
    "Example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 372408,
     "status": "ok",
     "timestamp": 1724837242681,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "dlW-gNuaSWn4",
    "outputId": "8bbded6e-5c64-4463-ef08-08ed10a520f3"
   },
   "outputs": [],
   "source": [
    "results = model.predict(source='/content/drive/MyDrive/YT Videos/ABA Therapy - Social Engagement.mp4', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1725480111571,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "257KUusuCaEX"
   },
   "outputs": [],
   "source": [
    " from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1725480677098,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "zkvwzCwCa8PG"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # We can use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  #Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/ABA Therapy - Social Engagement.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1976,
     "status": "ok",
     "timestamp": 1725480683470,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "Xa4jRqK2r_ej"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/ABA Therapy - Learning about Animals.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked1.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1845,
     "status": "ok",
     "timestamp": 1725480689214,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "hB-zZZrjAdJt"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/ABA Therapy_ Daniel - Communication.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked2.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2052,
     "status": "ok",
     "timestamp": 1725480697331,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "c2H8ca4uOJ5N"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Augmentative and Alternative Communication AAC.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked3.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1725480701726,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "Xx-dN6kTQIkU"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/MASS TRIAL (Gross motor imitation).mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked4.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1725480708582,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "dQKXESP9aR_d"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Group Therapy for Autism Spectrum Disorder.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked5.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 662,
     "status": "ok",
     "timestamp": 1725480760225,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "OHjaazkyXHCB"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Exploring the Therapeutic Playroom.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked7.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3697,
     "status": "ok",
     "timestamp": 1725480652891,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "UDibFqr_3qzl"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Natural Environment Teaching (NET).mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked8.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "adgam_MZMKrV",
    "outputId": "2bdbc963-86d3-4970-9b5b-b9d2de65dd68"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Preference Assessment with Toys_ Multiple Stimulus without Replacement (MSWO).mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked9.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1725480760226,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "RTFEWSmMPUtG"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Incidental Teaching.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked10.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1725480818940,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "sQ4LBNrbsVYa"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/videoplayback.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked11.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1725480981414,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "MUKkZ8Am3Tya"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Jan 5 SonRise Mom part 1.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked12.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzQ5Ln6lrwoe"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1725480921201,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "3161LWiOOnWy"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Matching.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked12.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCBq72JyoIEA"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Sensory Play at Home_ Proprioceptive Games.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked13.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CM6rFziG_VIb"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Discrete Trial Training.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked14.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyDHI4akmRQZ"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Play Therapy Session working on Feelings with Candy Land Game.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked15.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w37BSnZl8jDz",
    "outputId": "09e68a01-3b9b-438e-89eb-58394e5511cb"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/How to Do Play Therapy _ Building a Growth Mindset Role Play.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked17.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1725477619893,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "BVquve15bawD"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/videoplayback (1).mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked17.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 635,
     "status": "ok",
     "timestamp": 1725481221926,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "we0XkISEHb1z"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign child/therapist labels based on heuristic or manually as needed\n",
    "                label = \"Child\" if int(track_id) % 2 == 0 else \"Therapist\"  # Example heuristic\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Autism (Moderate - Severe) and ABA - Training Session.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked20.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvSKUSBHsE6L"
   },
   "source": [
    "Assigning the child and therapist labes manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 839,
     "status": "ok",
     "timestamp": 1725481172621,
     "user": {
      "displayName": "Mehtab Dawkar",
      "userId": "09280146918065202468"
     },
     "user_tz": -330
    },
    "id": "evvt5Pc2WGvB"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a dictionary to store track ID assignments (e.g., child or therapist)\n",
    "track_labels = {}\n",
    "\n",
    "# Function to assign roles to tracks\n",
    "def assign_label(track_id):\n",
    "    if track_id not in track_labels:\n",
    "        # Manual assignment: update this list based on your observations\n",
    "        if len(track_labels) == 0:\n",
    "            # Assume the first detected ID is the Child\n",
    "            track_labels[track_id] = \"Child\"\n",
    "        else:\n",
    "            # Assign remaining IDs based on context or initial assignments\n",
    "            track_labels[track_id] = \"Therapist\"\n",
    "    return track_labels[track_id]\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign labels to each track based on initial assignments or heuristics\n",
    "                label = assign_label(track_id)\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Jan 5 SonRise Mom part 1.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked19.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68333,
     "output_embedded_package_id": "1ECFFTgC1gF6dFXGqaZgSQwlS3xqm6kkw"
    },
    "id": "U90-MX6NIG9Q",
    "outputId": "e266b5ee-02b1-4bea-91c1-406e1b101462"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Use YOLOv8s.pt or other models if needed\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
    "\n",
    "# Define a dictionary to store track ID assignments (e.g., child or therapist)\n",
    "track_labels = {}\n",
    "\n",
    "# Function to assign roles to tracks\n",
    "def assign_label(track_id):\n",
    "    if track_id not in track_labels:\n",
    "        # Manual assignment: update this list based on your observations\n",
    "        if len(track_labels) == 0:\n",
    "            # Assume the first detected ID is the Child\n",
    "            track_labels[track_id] = \"Child\"\n",
    "        else:\n",
    "            # Assign remaining IDs based on context or initial assignments\n",
    "            track_labels[track_id] = \"Therapist\"\n",
    "    return track_labels[track_id]\n",
    "\n",
    "# Define a function to process the video and track persons\n",
    "def process_video(video_path, output_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Predict persons in the frame using YOLOv8\n",
    "        results = model.predict(source=frame, conf=0.4, classes=[0])  # Class 0 is for person detection\n",
    "\n",
    "        # Initialize empty list for detections\n",
    "        detections = []\n",
    "\n",
    "        # Extract detections (bounding boxes, confidences)\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                try:\n",
    "                    bbox = box.xyxy[0].cpu().numpy()  # Extract bounding box coordinates\n",
    "                    confidence = float(box.conf.cpu().numpy())  # Extract confidence\n",
    "                    class_id = int(box.cls.cpu().numpy())  # Extract class ID (person)\n",
    "\n",
    "                    # Ensure detection format: [bbox, confidence, class_id]\n",
    "                    if len(bbox) == 4:  # Ensure bbox has 4 coordinates\n",
    "                        detections.append((bbox, confidence, class_id))  # Append as a tuple\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing box: {e}\")\n",
    "\n",
    "        # Check if detections are not empty\n",
    "        if len(detections) > 0:\n",
    "            # Update tracker with new detections\n",
    "            tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            for track in tracks:\n",
    "                track_id = track.track_id\n",
    "                bbox = track.to_tlbr()  # Get bounding box in (top-left, bottom-right) format\n",
    "\n",
    "                # Assign labels to each track based on initial assignments or heuristics\n",
    "                label = assign_label(track_id)\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw ID and label (ensuring proper formatting)\n",
    "                cv2.putText(frame, f\"{label} ID: {track_id}\", (int(bbox[0]), max(0, int(bbox[1]) - 10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with overlay\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame for debugging (optional)\n",
    "        cv2_imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test with an example video path\n",
    "input_video_path = '/content/drive/MyDrive/YT Videos/Speech Therapy Training Session- Moderate to Severe Autism.mp4'\n",
    "output_video_path = '/content/drive/MyDrive/YT Videos/output_tracked20.mp4'\n",
    "process_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmBZlak4d2ev"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOkSB/QO9rX+n7CyFhKstpV",
   "gpuType": "T4",
   "mount_file_id": "1dq0XouSx9kx9RQMw4jn0KFJlV-2wW0FG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
